name: Test Azure
on:
  push:
    branches:
      - jackie-azure-storage-ci

jobs:
  pre-commit:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v2
      - uses: actions/setup-python@v2
      - uses: pre-commit/action@v2.0.3

  unit:
    name: Azure unit tests / Python ${{ matrix.ver }} on ${{ matrix.os }}
    runs-on: ${{ matrix.os }}
    strategy:
      fail-fast: false
      matrix:
        os: [ ubuntu-latest ]
        ver: [ '3.10' ]

    steps:
      - uses: actions/checkout@v2

      - name: Set up Python
        uses: actions/setup-python@v2
        with:
          python-version: ${{ matrix.ver }}

      - name: Install dependencies
        run: |
          python3 -m pip install requests azure-storage-blob azure-identity

      - name: Execute Azure unit tests
        env:
          METAFLOW_DATASTORE_SYSROOT_AZURE: ${{ secrets.METAFLOW_DATASTORE_SYSROOT_AZURE }}
          METAFLOW_AZURE_STORAGE_ACCOUNT_URL: ${{ secrets.METAFLOW_AZURE_STORAGE_ACCOUNT_URL }}
          METAFLOW_AZURE_STORAGE_SHARED_ACCESS_SIGNATURE: ${{ secrets.METAFLOW_AZURE_STORAGE_SHARED_ACCESS_SIGNATURE }}
        run: |
          export METAFLOW_DATASTORE_SYSROOT_AZURE=$METAFLOW_DATASTORE_SYSROOT_AZURE/ob-ci/azure_unit_tests/$(date +%s)-$(openssl rand -hex 5)
          cd test/unit && PYTHONPATH=$(pwd)/../../ python3 test_azure_storage.py
        # Unit test tries its best to clean up any blobs created...
        # TODO add catch all clean up of everything in METAFLOW_DATASTORE_SYSROOT_AZURE

  core:
    name: Azure core tests / Python ${{ matrix.ver }} on ${{ matrix.os }}
    runs-on: ${{ matrix.os }}
    strategy:
      fail-fast: false
      matrix:
        os: [ ubuntu-latest ]
        ver: [ '3.10' ]

    steps:
      - uses: actions/checkout@v2

      - name: Set up Python
        uses: actions/setup-python@v2
        with:
          python-version: ${{ matrix.ver }}

      - name: Install dependencies
        run: |
          python3 -m pip install pylint requests azure-storage-blob azure-identity

      - name: Execute Azure core tests
        env:
          METAFLOW_DATASTORE_SYSROOT_AZURE: ${{ secrets.METAFLOW_DATASTORE_SYSROOT_AZURE }}
          METAFLOW_AZURE_STORAGE_ACCOUNT_URL: ${{ secrets.METAFLOW_AZURE_STORAGE_ACCOUNT_URL }}
          METAFLOW_AZURE_STORAGE_SHARED_ACCESS_SIGNATURE: ${{ secrets.METAFLOW_AZURE_STORAGE_SHARED_ACCESS_SIGNATURE }}
        run: |
          export METAFLOW_DATASTORE_SYSROOT_AZURE=$METAFLOW_DATASTORE_SYSROOT_AZURE/ob-ci/azure_core_tests/$(date +%s)-$(openssl rand -hex 5)
          cd test/extensions
          sh install_packages.sh
          cd ../../
          cd test/core && PYTHONPATH=`pwd`/../../ python3 run_tests.py --num-parallel 8 --inherit-env --contexts python3-all-local-azure-storage
        # TODO add catch all clean up of everything in METAFLOW_DATASTORE_SYSROOT_AZURE

  k8s-and-argo:
    name: Azure / Argo + k8s / Python ${{ matrix.ver }} on ${{ matrix.os }}
    runs-on: ${{ matrix.os }}
    strategy:
      fail-fast: false
      matrix:
        os: [ ubuntu-latest ]
        ver: [ '3.10' ]

    steps:
      - uses: actions/checkout@v2

      - name: Check out metaflow-on-azure repo
        uses: actions/checkout@v2
        with:
          repository: Outerbounds/metaflow-on-azure
          ssh-key: ${{ secrets.METAFLOW_ON_AZURE_REPO_DEPLOY_KEY }}
          path: metaflow-on-azure

      - name: Set up Python
        uses: actions/setup-python@v2
        with:
          python-version: ${{ matrix.ver }}

      - name: Setup Conda
        run: |
          curl -sLO https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh
          bash Miniconda3-latest-Linux-x86_64.sh -b -p
          source $HOME/miniconda3/bin/activate
          conda config --add channels conda-forge
          conda config --set channel_priority strict

      - name: Install dependencies
        run: |
          source $HOME/miniconda3/bin/activate
          python3 -m pip install pylint requests azure-storage-blob azure-identity azure-cli
          python3 -m pip install kubernetes
          curl -sLO https://github.com/argoproj/argo-workflows/releases/download/v3.3.8/argo-linux-amd64.gz
          gunzip argo-linux-amd64.gz
          chmod +x argo-linux-amd64
          mv ./argo-linux-amd64 /usr/local/bin/argo
          argo version

      - name: Prepare k8s access
        env:
          AZURE_K8S_SERVICE_PRINCIPAL_APP_ID: ${{ secrets.AZURE_K8S_SERVICE_PRINCIPAL_APP_ID }}
          AZURE_K8S_SERVICE_PRINCIPAL_PASSWORD: ${{ secrets.AZURE_K8S_SERVICE_PRINCIPAL_PASSWORD }}
          AZURE_K8S_SERVICE_PRINCIPAL_TENANT_ID: ${{ secrets.AZURE_K8S_SERVICE_PRINCIPAL_TENANT_ID }}
          AZURE_K8S_CLUSTER_NAME: metaflow-kubernetes-ci
          AZURE_K8S_CLUSTER_RESOURCE_GROUP: rg-k8s-metaflow-ci
        run: |
          az login -o none --service-principal --username "$AZURE_K8S_SERVICE_PRINCIPAL_APP_ID" --password "$AZURE_K8S_SERVICE_PRINCIPAL_PASSWORD" --tenant "$AZURE_K8S_SERVICE_PRINCIPAL_TENANT_ID"
          az aks get-credentials --resource-group "$AZURE_K8S_CLUSTER_RESOURCE_GROUP" --name "$AZURE_K8S_CLUSTER_NAME"

      - name: Kick off port forward to Metaflow services in AKS
        run: |
          python3 metaflow-on-azure/scripts/forward_metaflow_ports.py --include-argo &
          echo "Waiting 10 seconds for port forwards to be ready"
          sleep 10
        # TODO add catch all clean up of everything in METAFLOW_DATASTORE_SYSROOT_AZURE

      - name: Run a Flow on Kubernetes
        env:
          METAFLOW_DATASTORE_SYSROOT_AZURE: ${{ secrets.METAFLOW_DATASTORE_SYSROOT_AZURE }}
          METAFLOW_AZURE_STORAGE_ACCOUNT_URL: ${{ secrets.METAFLOW_AZURE_STORAGE_ACCOUNT_URL }}
          METAFLOW_AZURE_STORAGE_SHARED_ACCESS_SIGNATURE: ${{ secrets.METAFLOW_AZURE_STORAGE_SHARED_ACCESS_SIGNATURE }}
          METAFLOW_KUBERNETES_SECRETS: secret-for-ci-run-${{ github.run_id }}.${{ github.run_attempt }}-step-k8s
          METAFLOW_SERVICE_URL: http://127.0.0.1:8080/
          METAFLOW_SERVICE_INTERNAL_URL: http://metadata-service.default:8080/
        run: |
          source $HOME/miniconda3/bin/activate
          export METAFLOW_DATASTORE_SYSROOT_AZURE=$(cat .metaflow_datastore_sysroot_azure)
          export PYTHONPATH=$(pwd)
          kubectl create secret generic "$METAFLOW_KUBERNETES_SECRETS" --from-literal=METAFLOW_AZURE_STORAGE_SHARED_ACCESS_SIGNATURE="${METAFLOW_AZURE_STORAGE_SHARED_ACCESS_SIGNATURE}"
          python3 metaflow-on-azure/sample_flows/conda_noop_flow.py --environment=conda --datastore=azure --metadata=service run --with=kubernetes
          kubectl delete secret "$METAFLOW_KUBERNETES_SECRETS"

      - name: Run a Flow on Argo
        env:
          METAFLOW_DATASTORE_SYSROOT_AZURE: ${{ secrets.METAFLOW_DATASTORE_SYSROOT_AZURE }}
          METAFLOW_AZURE_STORAGE_ACCOUNT_URL: ${{ secrets.METAFLOW_AZURE_STORAGE_ACCOUNT_URL }}
          METAFLOW_AZURE_STORAGE_SHARED_ACCESS_SIGNATURE: ${{ secrets.METAFLOW_AZURE_STORAGE_SHARED_ACCESS_SIGNATURE }}
          METAFLOW_KUBERNETES_SECRETS: secret-for-ci-run-${{ github.run_id }}.${{ github.run_attempt }}-step-argo
          METAFLOW_SERVICE_URL: http://127.0.0.1:8080/
          METAFLOW_SERVICE_INTERNAL_URL: http://metadata-service.default:8080/
          METAFLOW_KUBERNETES_NAMESPACE: argo
        run: |
          source $HOME/miniconda3/bin/activate
          export METAFLOW_DATASTORE_SYSROOT_AZURE=$(cat .metaflow_datastore_sysroot_azure)
          export PYTHONPATH=$(pwd)
          kubectl create secret generic "$METAFLOW_KUBERNETES_SECRETS" -n "${METAFLOW_KUBERNETES_NAMESPACE}" --from-literal=METAFLOW_AZURE_STORAGE_SHARED_ACCESS_SIGNATURE="${METAFLOW_AZURE_STORAGE_SHARED_ACCESS_SIGNATURE}"
          python3 metaflow-on-azure/sample_flows/conda_noop_flow.py --environment=conda --datastore=azure --metadata=service --with retry argo-workflows create
          python3 metaflow-on-azure/sample_flows/conda_noop_flow.py --environment=conda --datastore=azure --metadata=service argo-workflows trigger --run-id-file run_id_file
          argo wait --verbose -n argo "$(cat run_id_file | cut -f2- -d-)"
          kubectl delete secret "$METAFLOW_KUBERNETES_SECRETS" -n "${METAFLOW_KUBERNETES_NAMESPACE}"
        # TODO add catch all clean up of everything in METAFLOW_DATASTORE_SYSROOT_AZURE


